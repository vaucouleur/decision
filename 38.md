Here’s the “production-feel” polish in a minimal, clean way:

1. A **SharedTermOracle** owned by the engine (so “what is shared?” is centralized and stable).
2. An **export throttle** so UF/DL don’t re-export the same equalities every iteration (epoch-based, no hashset bloat).

Both are small changes that massively reduce churn.

---

# 1) SharedTermOracle (engine-owned)

### `crates/smt-engine/src/shared_terms.rs`

```rust
#![forbid(unsafe_code)]

use hashbrown::{HashMap, HashSet};
use rustc_hash::FxHasher;
use core::hash::BuildHasherDefault;

use smt_core::TermId;

use crate::atoms::AtomTable;
use crate::theory::Theory;

type FxBuild = BuildHasherDefault<FxHasher>;

/// Central place to compute and store which terms are considered "shared"
/// across theories for Nelson–Oppen-like equality exchange.
pub struct SharedTermOracle {
    shared: HashSet<TermId, FxBuild>,
    epoch: u64,
}

impl Default for SharedTermOracle {
    fn default() -> Self {
        Self { shared: HashSet::default(), epoch: 0 }
    }
}

impl SharedTermOracle {
    pub fn epoch(&self) -> u64 { self.epoch }

    pub fn is_shared(&self, t: TermId) -> bool {
        self.shared.contains(&t)
    }

    pub fn shared_set(&self) -> &HashSet<TermId, FxBuild> {
        &self.shared
    }

    /// Recompute shared terms from current atoms and theory endpoint extraction.
    pub fn recompute(&mut self, atoms: &AtomTable, theories: &mut [Box<dyn Theory>]) {
        // term -> owner bitmask (up to 32 theories)
        let mut owners: HashMap<TermId, u32, FxBuild> = HashMap::default();

        for atom in atoms.iter_atoms() {
            let th_idx = atom.theory.0;
            let bit = 1u32 << (th_idx.min(31) as u32);

            for t in theories[th_idx].atom_endpoints(atom.term) {
                owners.entry(t).and_modify(|m| *m |= bit).or_insert(bit);
            }
        }

        self.shared.clear();
        for (t, mask) in owners {
            if mask.count_ones() >= 2 {
                self.shared.insert(t);
            }
        }

        self.epoch = self.epoch.wrapping_add(1);
    }
}
```

### Engine field

Add to `SmtEngine`:

```rust
shared_terms: crate::shared_terms::SharedTermOracle,
```

Initialize as `Default::default()`.

### When to recompute?

Recompute whenever the atom table grows (new atoms discovered) — that’s the main time shared-ness changes. A simple heuristic:

* Keep `last_atom_count`
* If `atoms.len() != last_atom_count`, recompute

```rust
fn maybe_recompute_shared_terms(&mut self) {
    let n = self.atoms.len();
    if n != self.last_atom_count {
        self.shared_terms.recompute(&self.atoms, &mut self.theories);
        self.last_atom_count = n;
    }
}
```

Call it near the top of each outer solve iteration, and also after propositionalization if you add atoms incrementally.

---

# 2) Export throttle: per-theory epoch + “already exported” watermark

Without throttling, UF and DL might export the same equalities repeatedly every time the engine asks, even if nothing changed.

A lightweight pattern:

* Engine has a global `export_epoch: u64` that increments when something relevant changes:

  * new theory assignment added
  * new equality imported
  * backtrack levels changed
* Each theory keeps `last_export_epoch: u64`
* If `last_export_epoch == export_epoch`, export nothing

This is minimal and fast.

### Engine field

```rust
export_epoch: u64,
```

Increment it when:

* SAT assigns a new literal (after notify)
* a theory imports an equality (during equality-sharing broadcast)
* backtrack occurs (levels pop)

You can be conservative: increment at the start of each outer loop; it still throttles within the inner fixpoint loop.

### Theory-side watermark

In UF and DL, add:

```rust
last_export_epoch: u64,
```

and expose a tiny method or just store it internally.

### EqualitySharing signature improvement

Instead of passing the whole shared set and tcx only, also pass oracle epoch + engine export epoch:

```rust
fn export_equalities(&mut self, oracle: &SharedTermOracle, export_epoch: u64, tcx: &mut TheoryCtx) -> Vec<SharedEq>;
```

Then in theory:

```rust
if self.last_export_epoch == export_epoch {
    return Vec::new();
}
self.last_export_epoch = export_epoch;
```

This prevents duplicates within a single stable state.

---

# 3) Even better (still small): dedup exported equalities by pair key

Epoch throttling prevents churn, but you can still export duplicates across different epochs if nothing really changed.

Add a tiny cache keyed by `(min(a,b), max(a,b))` per theory and cleared on backtrack scope/level pop.

### In UF/DL:

```rust
exported_pairs: hashbrown::HashSet<(TermId, TermId), FxBuild>,
```

When exporting:

```rust
let (p, q) = if a < b { (a,b) } else { (b,a) };
if !self.exported_pairs.insert((p,q)) { continue; }
out.push(...)
```

On `push_level/push_scope`: checkpoint size; on pop: truncate (or clear fully for simplicity).

This is very effective and still easy.

---

# 4) How it fits together (engine loop snippet)

```rust
loop {
    self.maybe_recompute_shared_terms();
    self.export_epoch = self.export_epoch.wrapping_add(1);

    // SAT propagate + notify
    self.sat.propagate()?;
    self.sync_levels_with_sat();
    self.notify_theories_new_trail();

    // theory fixpoint
    self.theory_fixpoint()?;

    // equality sharing
    self.equality_sharing_round()?;

    // another fixpoint because imports can unlock new props/conflicts
    self.theory_fixpoint()?;

    // decide / terminate...
}
```

Inside `equality_sharing_round`, pass `&self.shared_terms` and `self.export_epoch`.

---

# 5) Why this feels “beautiful”

* Shared-term logic is centralized and reproducible.
* Theories don’t guess global state; they consume an oracle.
* Exporting is stable (no repeated emissions), which makes debugging and perf sane.
* It scales naturally as you add more theories.

---

If you say **yes**, I’ll give you concrete code for:

* `SmtEngine::equality_sharing_round()` with oracle + epoch,
* UF and DL “exported_pairs” checkpoint/rollback integration (so it’s correct under push/pop and backtracking).
